{
  "model": "mistral_7b",
  "experiment": "exp2",
  "test_samples": 968,
  "metrics": {
    "bleu_1": 0.09026058163648068,
    "bleu_2": 0.03397650910632544,
    "bleu_3": 0.015237547880360277,
    "bleu_4": 0.00784919231819252,
    "rouge_1_f1": 0.16388810463005798,
    "rouge_2_f1": 0.03152133235743089,
    "rouge_l_f1": 0.0961863533200268,
    "meteor": 0.10718892351728308,
    "bertscore_f1": 0.0,
    "avg_reference_length": 89.91528925619835,
    "avg_candidate_length": 161.52479338842974,
    "length_ratio": 1.796410763114961,
    "length_difference": 71.6095041322314
  },
  "metrics_by_language": {
    "english": {
      "rouge_1_f1": 0.31591794006542556,
      "n": 331
    },
    "hindi": {
      "rouge_1_f1": 0.0351564689246138,
      "n": 311
    },
    "code_mixed": {
      "rouge_1_f1": 0.13233492418615148,
      "n": 326
    }
  },
  "metrics_by_complexity": {
    "professional": {
      "rouge_1_f1": 0.19091706704948572,
      "n": 318
    },
    "intermediate": {
      "rouge_1_f1": 0.1598629407493177,
      "n": 326
    },
    "layman": {
      "rouge_1_f1": 0.14140968912309287,
      "n": 324
    }
  },
  "checkpoint": "models/mistral_7b/checkpoints/exp2/pretrained/final"
}