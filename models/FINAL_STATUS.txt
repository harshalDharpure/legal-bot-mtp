================================================================================
TRAINING STATUS - FINAL SUMMARY
================================================================================

âœ… COMPLETED MODELS:
  1. MuRIL-Large        â†’ GPU 2 âœ… COMPLETED
  2. XLM-RoBERTa-Large  â†’ GPU 1 âœ… COMPLETED

ðŸ”„ TRAINING IN PROGRESS:
  3. mT5-Large          â†’ GPU 0 (restarted with batch_size=1)
  4. FLAN-T5-XL         â†’ GPU 3 (restarted with batch_size=1)

================================================================================
GPU ALLOCATION:
================================================================================
GPU 0: mT5-Large (batch_size=1, gradient_accumulation=16)
GPU 1: XLM-RoBERTa-Large âœ… COMPLETED
GPU 2: MuRIL-Large âœ… COMPLETED  
GPU 3: FLAN-T5-XL (batch_size=1, gradient_accumulation=32)
GPU 4: Available

================================================================================
MONITORING COMMANDS:
================================================================================
Check status:     python3 models/check_status.py
View logs:        tail -f models/{model}/logs/training_gpu*.log
GPU usage:        watch -n 1 nvidia-smi
All processes:    ps aux | grep train.py

================================================================================
NOTES:
================================================================================
- Batch sizes reduced to 1 for large models to avoid OOM
- Gradient accumulation increased to maintain effective batch size
- Training continues in background
- Checkpoints saved in: models/{model}/checkpoints/
- Logs saved in: models/{model}/logs/

================================================================================
