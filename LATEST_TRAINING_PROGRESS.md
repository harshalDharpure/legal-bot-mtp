# üìä Latest Training Progress - Experiment 1 (Finetuning Only)

**Last Updated**: $(date)  
**Experiment**: Exp1 - Finetuning Only (Baseline)  
**Dataset**: 70/10/20 split (3,255 train / 454 val / 968 test)

---

## ‚úÖ **COMPLETED MODELS** (5/7 - 71%)

### 1. ‚úÖ **Qwen2.5-7B** - COMPLETED
- **Status**: ‚úÖ Training completed successfully
- **Checkpoint**: `models/qwen2.5_7b/checkpoints/exp1/final`
- **Final Loss**: 1.379
- **Training Time**: ~1.4 hours (5,067 seconds)
- **Method**: QLoRA (4-bit quantization)
- **GPU**: Single GPU 0

### 2. ‚úÖ **LLaMA-3.1-8B** - COMPLETED
- **Status**: ‚úÖ Training completed successfully
- **Checkpoint**: `models/llama3.1_8b/checkpoints/exp1/final`
- **Final Loss**: 1.39
- **Training Time**: ~1.7 hours (5,963 seconds)
- **Method**: QLoRA (4-bit quantization)
- **GPU**: Single GPU 1

### 3. ‚úÖ **Mistral-7B** - COMPLETED
- **Status**: ‚úÖ Training completed successfully
- **Checkpoint**: `models/mistral_7b/checkpoints/exp1/final`
- **Final Loss**: 1.039
- **Training Time**: ~1.3 hours (4,759 seconds)
- **Method**: QLoRA (4-bit quantization)
- **GPU**: Single GPU 4

### 4. ‚úÖ **Phi-3-mini** - COMPLETED
- **Status**: ‚úÖ Training completed successfully (FP16 issue fixed)
- **Checkpoint**: `models/phi3_mini/checkpoints/exp1/final`
- **Final Loss**: 1.779
- **Training Time**: ~2.7 hours (9,730 seconds)
- **Method**: QLoRA (4-bit quantization) + FP16 disabled
- **GPU**: Single GPU 4

### 5. ‚úÖ **Qwen2.5-1.5B** - COMPLETED
- **Status**: ‚úÖ Training completed successfully
- **Checkpoint**: `models/qwen2.5_1.5b/checkpoints/exp1/final`
- **Method**: Full fine-tuning

---

## ‚ùå **PENDING MODELS** (2/7 - 29%)

### 6. ‚ùå **XLM-RoBERTa-Large** - NOT STARTED
- **Status**: ‚è≥ Waiting to start
- **Method**: Full fine-tuning (generation task)

### 7. ‚ùå **MuRIL-Large** - NOT STARTED
- **Status**: ‚è≥ Waiting to start
- **Method**: Full fine-tuning (generation task)

---

## üìà **Summary**
- **Completed**: 5/7 models (71%)
- **In Progress**: 0/7 models
- **Pending**: 2/7 models (29%)
- **All Large LLMs Completed**: ‚úÖ Yes (Qwen2.5-7B, LLaMA-3.1-8B, Mistral-7B, Phi-3-mini)

---

## üéØ **Next Steps**
1. Train XLM-RoBERTa-Large on Exp1
2. Train MuRIL-Large on Exp1
3. Evaluate all completed models on test set
