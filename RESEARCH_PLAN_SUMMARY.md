# ğŸ¯ Research Plan Summary - Quick Reference

## Current Status âœ…

**Completed:**
- Classification task (complexity prediction)
- 80/20 train/test split
- 2 models trained (XLM-RoBERTa, MuRIL)
- Zero-shot and few-shot experiments (classification)

**Missing (Critical):**
- âŒ Generation task (response generation)
- âŒ Validation set (70/10/20 split)
- âŒ Generation models (LLaMA, Mistral, Qwen, Phi-3)
- âŒ Generation metrics (BLEU, ROUGE, METEOR)
- âŒ Pretraining experiments

---

## ğŸ¯ Research Plan (4 Phases)

### Phase 1: Dataset Preparation
**Goal**: Create 70/10/20 train/val/test split

**Actions:**
1. Load all 1,200 samples
2. Stratified split (language, complexity, bucket)
3. Create train (840), val (120), test (240) files
4. Convert to generation format (user query â†’ assistant response)

**Output**: `data/splits/train_70.jsonl`, `val_10.jsonl`, `test_20.jsonl`

---

### Phase 2: Model Setup
**Goal**: Setup 7 generation models

**Models:**
1. **LLaMA-3.1-8B** (QLoRA)
2. **Mistral-7B** (QLoRA)
3. **Qwen2.5-7B** (QLoRA)
4. **Qwen2.5-1.5B** (Full fine-tuning)
5. **Phi-3-mini** (Full fine-tuning)
6. **XLM-RoBERTa-Large** (Retrain for generation)
7. **MuRIL-Large** (Retrain for generation)

**Output**: Model folders with configs and training scripts

---

### Phase 3: Experiments

#### Exp1: Supervised Baseline (70/10/20)
- Train all 7 models
- Evaluate on test set
- Generate Table 1 (Overall Performance)

#### Exp2: Pretraining + Finetuning
- **Exp2**: Pretrain on legal corpus (LLaMA, Mistral, Qwen2.5-7B)
- **Exp1**: Finetune pretrained models on dialogue data
- Compare: Pretraining+Finetuning vs Finetuning only

#### Exp3: Zero-Shot Transfer
- Train on 2 languages â†’ Test on 3rd
- Generate Table 2 (Language-Specific Performance)

#### Exp4: Few-Shot Learning
- Train with 5, 10, 20, 50 examples
- Analyze performance vs shot size

---

### Phase 4: Evaluation & Tables

**Metrics to Calculate:**
- BLEU-1, BLEU-2, BLEU-3, BLEU-4
- ROUGE-1 F1, ROUGE-2 F1, ROUGE-L F1
- METEOR
- Response Length (avg reference, avg candidate, ratio, difference)

**Tables to Generate:**
1. **Table 1**: Overall Performance Metrics (all models, all metrics)
2. **Table 2**: Language-Specific Performance (ROUGE-1 F1 by language)
3. **Table 3**: Complexity-Specific Performance (ROUGE-1 F1 by complexity)
4. **Table 4**: Response Length Comparison
5. **Table 5**: Model Ranking Summary (ROUGE-1 F1)

---

## ğŸ“Š Expected Results (Based on Image)

**Best Model (LLaMA-3.1-8B):**
- BLEU-1: ~0.32
- BLEU-4: ~0.05
- ROUGE-1 F1: ~0.36
- ROUGE-L F1: ~0.23
- METEOR: ~0.37

**Language Performance (ROUGE-1 F1):**
- English: ~0.42
- Hindi: ~0.32
- Code-mixed: ~0.33

---

## ğŸš€ Next Steps (Priority Order)

1. **âœ… Create 70/10/20 split** (Day 1)
2. **âœ… Setup generation models** (Day 2-3)
3. **âœ… Train Exp1 (Supervised Baseline)** (Day 4-8)
4. **âœ… Train Exp2 (Pretraining + Finetuning)** (Day 9-12)
5. **âœ… Train Exp3 (Zero-Shot)** (Day 13-14)
6. **âœ… Train Exp4 (Few-Shot)** (Day 15)
7. **âœ… Evaluate all models** (Day 16-17)
8. **âœ… Generate tables** (Day 18)
9. **âœ… Documentation** (Day 19-20)

---

## ğŸ“ Repository Structure

```
legal-bot/
â”œâ”€â”€ data/splits/              # 70/10/20 split
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ exp1_supervised/      # 70/10/20 baseline
â”‚   â”œâ”€â”€ exp2_pretraining/     # Pretraining + finetuning
â”‚   â”œâ”€â”€ exp3_zeroshot/        # Zero-shot transfer
â”‚   â””â”€â”€ exp4_fewshot/         # Few-shot learning
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ llama3.1_8b/          # Each model in separate folder
â”‚   â”œâ”€â”€ mistral_7b/
â”‚   â”œâ”€â”€ qwen2.5_7b/
â”‚   â”œâ”€â”€ qwen2.5_1.5b/
â”‚   â”œâ”€â”€ phi3_mini/
â”‚   â”œâ”€â”€ xlmr_large/           # Retrain for generation
â”‚   â””â”€â”€ muril_large/          # Retrain for generation
â””â”€â”€ results/
    â””â”€â”€ tables/               # Paper-ready tables
```

---

## âš ï¸ Critical Notes

1. **Task Shift**: Classification â†’ Generation
   - Need to convert dialogue pairs to input/output format
   - Format: `"User: {query}\nAssistant: {response}"`

2. **Test Set**: Hide complete dialogue pairs
   - No overlap between train/val/test
   - Use validation set for hyperparameter tuning
   - Test set only for final evaluation

3. **GPU Constraints**: Use QLoRA for large models
   - LLaMA-3.1-8B: QLoRA (4-bit)
   - Mistral-7B: QLoRA (4-bit)
   - Qwen2.5-7B: QLoRA (4-bit)
   - Small models: Full fine-tuning

4. **Reproducibility**: Random seed 42, save all checkpoints

---

## ğŸ“ Paper Title Suggestion

**"Multilingual Legal Dialogue Generation: Zero-Shot Learning for POCSO Act Queries"**

**Alternative:**
**"Cross-Lingual Legal Response Generation: A Comprehensive Study on POCSO Act Dialogues"**

---

**Status**: ğŸŸ¢ Ready to implement  
**Start**: Create 70/10/20 dataset split
